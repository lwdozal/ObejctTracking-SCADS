{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 03:42:04.044469: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-17 03:42:04.044552: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-17 03:42:04.934340: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-17 03:42:06.272606: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-17 03:42:11.423590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-vision-128k-instruct:\n",
      "- configuration_phi3_v.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.gemma2.configuration_gemma2 because of the following error (look up to see its traceback):\nNo module named 'transformers.models.gemma2.configuration_gemma2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/utils/import_utils.py:1560\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1560\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   1561\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1140\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models.gemma2.configuration_gemma2'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ladozal/efs/home/lwdozal/video_recognition/LLMs_object_tracking/llm_phi3_dashcam.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ws-ladozal.ncsu-las.net/home/ladozal/efs/home/lwdozal/video_recognition/LLMs_object_tracking/llm_phi3_dashcam.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbase64\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ws-ladozal.ncsu-las.net/home/ladozal/efs/home/lwdozal/video_recognition/LLMs_object_tracking/llm_phi3_dashcam.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m model_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmicrosoft/Phi-3-vision-128k-instruct\u001b[39m\u001b[39m\"\u001b[39m \n\u001b[0;32m---> <a href='vscode-notebook-cell://ws-ladozal.ncsu-las.net/home/ladozal/efs/home/lwdozal/video_recognition/LLMs_object_tracking/llm_phi3_dashcam.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_id, device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m, trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, torch_dtype\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m, _attn_implementation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mflash_attention_2\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m# use _attn_implementation='eager' to disable flash attention\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ws-ladozal.ncsu-las.net/home/ladozal/efs/home/lwdozal/video_recognition/LLMs_object_tracking/llm_phi3_dashcam.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='eager') # use _attn_implementation='eager' to disable flash attention\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ws-ladozal.ncsu-las.net/home/ladozal/efs/home/lwdozal/video_recognition/LLMs_object_tracking/llm_phi3_dashcam.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m processor \u001b[39m=\u001b[39m AutoProcessor\u001b[39m.\u001b[39mfrom_pretrained(model_id, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:541\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m has_remote_code \u001b[39m=\u001b[39m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mauto_map\n\u001b[1;32m    540\u001b[0m has_local_code \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys()\n\u001b[0;32m--> 541\u001b[0m trust_remote_code \u001b[39m=\u001b[39m resolve_trust_remote_code(\n\u001b[1;32m    542\u001b[0m     trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n\u001b[1;32m    543\u001b[0m )\n\u001b[1;32m    545\u001b[0m \u001b[39m# Set the adapter kwargs\u001b[39;00m\n\u001b[1;32m    546\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39madapter_kwargs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m adapter_kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:752\u001b[0m, in \u001b[0;36mkeys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mkeys\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    751\u001b[0m     mapping_keys \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 752\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_attr_from_module(key, name)\n\u001b[1;32m    753\u001b[0m         \u001b[39mfor\u001b[39;00m key, name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config_mapping\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    754\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m    755\u001b[0m     ]\n\u001b[1;32m    756\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping_keys \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extra_content\u001b[39m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:753\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mkeys\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    751\u001b[0m     mapping_keys \u001b[39m=\u001b[39m [\n\u001b[1;32m    752\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_attr_from_module(key, name)\n\u001b[0;32m--> 753\u001b[0m         \u001b[39mfor\u001b[39;00m key, name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config_mapping\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    754\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m    755\u001b[0m     ]\n\u001b[1;32m    756\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping_keys \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extra_content\u001b[39m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:749\u001b[0m, in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:693\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m attr)\n\u001b[1;32m    692\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(module, attr):\n\u001b[0;32m--> 693\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(module, attr)\n\u001b[1;32m    694\u001b[0m \u001b[39m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[39m# object at the top level.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m transformers_module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39mtransformers\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/utils/import_utils.py:1550\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[1;32m   1549\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m-> 1550\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[1;32m   1551\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/utils/import_utils.py:1562\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1560\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m   1561\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1563\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1564\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1565\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.gemma2.configuration_gemma2 because of the following error (look up to see its traceback):\nNo module named 'transformers.models.gemma2.configuration_gemma2'"
     ]
    }
   ],
   "source": [
    "from PIL import Image \n",
    "import requests \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n",
    "from transformers import AutoProcessor \n",
    "import base64\n",
    "\n",
    "model_id = \"microsoft/Phi-3-vision-128k-instruct\" \n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='flash_attention_2') # use _attn_implementation='eager' to disable flash attention\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='eager') # use _attn_implementation='eager' to disable flash attention\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [ \n",
    "#     {\"role\": \"user\", \"content\": \"<|image_1|>\\nIdentify all the cars in the image and their pixel location, including visible vehicles in the background.\"}]\n",
    "\n",
    "\n",
    "messages = [ \n",
    "    {\"role\": \"user\", \"content\": \"<|image_1|>\\nCount all the cars in the image and describe them. If possible include their pixel locations.\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./LMMs_object_tracking/dashcam/rgb-images/sample_images/image1.jpg\" \n",
    "\n",
    "image = Image.open(r\"../LLMs_object_tracking/dashcam/rgb-images/sample_images/image1.jpg\")\n",
    "image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ladozal/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are at least 10 cars visible in the image. They are of various colors including yellow, white, and black. The pixel locations for each car cannot be accurately determined due to the image's resolution and the angle of the camera.\n"
     ]
    }
   ],
   "source": [
    "prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \n",
    "# inputs = processor(prompt, [image], return_tensors=\"pt\")\n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n",
    "\n",
    "# remove input tokens \n",
    "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of single image\n",
    "phi3 cannot identify anything when using the prompt, *Identify all the cars in the image and their pixel location, including visible vehicles in the background.*\n",
    "\n",
    "It will find cars with the prompt: *Identify all the cars in the image.*:\n",
    "- There are multiple cars in the image, including a yellow taxi in the foreground and various other vehicles in the background.\n",
    "\n",
    "It will count cars pretty well with the prompt: *Count all the cars in the image.*:\n",
    "- There are at least 10 cars visible in the image.\n",
    "\n",
    "It will count the cars and describe them separately. Not good at pixel location using *ount all the cars in the image and describe them. If possible include their pixel locations.* :\n",
    "- There are at least 10 cars visible in the image. They are of various colors including yellow, white, and black. The pixel locations for each car cannot be accurately determined due to the image's resolution and the angle of the camera.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scads2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
