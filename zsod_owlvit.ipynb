{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "\n",
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== Add your Token here ===========================\n",
    "\n",
    "%env API_TOKEN=\n",
    "\n",
    "# ==========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "import requests\n",
    "#from IPython.display import Image\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"LAS_API_TOKEN\"),\n",
    ")\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpg_file = \"dashcam/rgb-images/sample_images/image1.jpg\"\n",
    "botsort_file = \"dashcam/rgb-images/sample_images_oldModel/image1.jpg\"\n",
    "vehicle_image = Image.open(jpg_file)\n",
    "texts = [[\"a photo of a car\", \"a photo of a motor bike\"]]\n",
    "inputs = processor(text=texts, images=vehicle_image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "target_sizes = torch.Tensor([vehicle_image.size[::-1]])\n",
    "# Convert outputs (bounding boxes and class logits) to COCO API\n",
    "results = processor.post_process_object_detection(outputs=outputs, threshold=0.1, target_sizes=target_sizes)\n",
    "\n",
    "i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "text = texts[i]\n",
    "boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "location_in_frame = []\n",
    "boxes_new = []\n",
    "scores_new = []\n",
    "labels_new = []\n",
    "\n",
    "for box in boxes:\n",
    "    box = [round(loc, 2) for loc in box.tolist()]\n",
    "    boxes_new.append(box)\n",
    "\n",
    "for score in scores:\n",
    "    score = round(score.item(), 3)\n",
    "    scores_new.append(score)\n",
    "\n",
    "for label in labels:\n",
    "    label = text[label]\n",
    "    labels_new.append(label)\n",
    "\n",
    "outputs = [\n",
    "    {'boxes': box, 'scores': score, 'labels': label}\n",
    "    for box, score, label in zip(boxes_new, scores_new, labels_new)\n",
    "]\n",
    "\n",
    "#This gives us the 10 nearest boxes to the dashboard, and excludes when the algo detects the dashboard\n",
    "filtered_list = list(filter(lambda d: d['boxes'][3] < 1200, outputs)) \n",
    "sorted_list = sorted(filtered_list, key=lambda x: x['boxes'][3], reverse=True)\n",
    "\n",
    "#send image to gpt4o\n",
    "for i in sorted_list:\n",
    "    bbox = i['boxes']\n",
    "    vehicle_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n",
    "    cropped_image = vehicle_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n",
    "    cropped_image.save('image_crop.jpg', quality=95)\n",
    "\n",
    "    # Getting the base64 string\n",
    "    base64_image = encode_image(\"image_crop.jpg\")\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"what colour and what type of motor vehicle is this, four words or less\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": \n",
    "                 {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "                }\n",
    "            ]}],\n",
    "            model=\"gpt-4o\",\n",
    "            )\n",
    "    i['lmm_description'] = chat_completion.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': [214.63, 765.04, 847.25, 1074.26], 'scores': 0.301, 'labels': 'a photo of a car', 'lmm_description': 'Green autorickshaw.'}\n",
      "{'boxes': [1221.19, 811.12, 1473.65, 1016.53], 'scores': 0.217, 'labels': 'a photo of a car', 'lmm_description': 'Yellow sedan taxi'}\n",
      "{'boxes': [862.17, 802.39, 1201.69, 1013.84], 'scores': 0.298, 'labels': 'a photo of a car', 'lmm_description': 'Yellow taxi cab.'}\n",
      "{'boxes': [1221.7, 809.84, 1465.12, 992.51], 'scores': 0.149, 'labels': 'a photo of a car', 'lmm_description': 'Yellow sedan taxi'}\n",
      "{'boxes': [1088.32, 803.21, 1205.94, 889.21], 'scores': 0.186, 'labels': 'a photo of a car', 'lmm_description': 'Brown SUV'}\n",
      "{'boxes': [914.01, 801.21, 1161.29, 888.07], 'scores': 0.156, 'labels': 'a photo of a car', 'lmm_description': 'Yellow taxi cab.'}\n",
      "{'boxes': [1173.08, 794.93, 1270.72, 874.43], 'scores': 0.107, 'labels': 'a photo of a car', 'lmm_description': 'Black sedan.'}\n",
      "{'boxes': [1091.03, 792.67, 1196.95, 858.73], 'scores': 0.361, 'labels': 'a photo of a car', 'lmm_description': 'Silver SUV'}\n",
      "{'boxes': [1173.73, 773.38, 1277.27, 853.66], 'scores': 0.311, 'labels': 'a photo of a car', 'lmm_description': 'Black pickup truck'}\n",
      "{'boxes': [1276.31, 795.68, 1394.03, 831.31], 'scores': 0.206, 'labels': 'a photo of a car', 'lmm_description': 'Yellow school bus'}\n",
      "{'boxes': [991.36, 769.12, 1098.05, 814.34], 'scores': 0.114, 'labels': 'a photo of a car', 'lmm_description': 'White SUV'}\n",
      "{'boxes': [1083.04, 770.74, 1167.07, 810.46], 'scores': 0.154, 'labels': 'a photo of a car', 'lmm_description': 'White sedan.'}\n"
     ]
    }
   ],
   "source": [
    "for i in sorted_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bounding_box(image, sorted_list):\n",
    "    plotted_image = ImageDraw.Draw(image)\n",
    "    for i in sorted_list:\n",
    "        box_new = i['boxes']\n",
    "        x0 = box_new[0]\n",
    "        y0 = box_new[1]\n",
    "        x1 = box_new[2]\n",
    "        y1 = box_new[3]\n",
    "        plotted_image.rectangle(((x0,y0), (x1,y1)),outline=(255,188,0),width=2)\n",
    "        plotted_image.text((x0, y0), i['lmm_description'],(255,188,0),font=ImageFont.truetype(font=\"arial\",size=34)) \n",
    "    plt.figure(figsize = (36,18))\n",
    "    plt.imshow(np.array(image))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the image\n",
    "plot_bounding_box(vehicle_image,sorted_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(botsort_file)\n",
    "plt.figure(figsize = (36,18))\n",
    "plt.imshow(np.array(image))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three edge cases\n",
    "#1 - unknown objects e.g. fuel tanks, tuk-tuks\n",
    "# solved with this approach\n",
    "#2 - temporary obsfucation\n",
    "#3 - class confusion e.g. truck112,car937,truck114,car948\n",
    "# solved with this approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scads2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
